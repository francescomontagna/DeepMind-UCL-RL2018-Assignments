{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment_1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
          "timestamp": 1517174839485
        },
        {
          "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
          "timestamp": 1515086437469
        },
        {
          "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
          "timestamp": 1511098107887
        }
      ],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "pYs6LMEbNqoQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RL homework 1\n",
        "**Due date: 19 February 2017, 11:55pm (just before mid-night!)**"
      ]
    },
    {
      "metadata": {
        "id": "6Sns0IKYNtsA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as studentnumber_DL_hw2.ipynb before the deadline above.\n",
        "\n",
        "Also send a sharable link to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you."
      ]
    },
    {
      "metadata": {
        "id": "9v_SYckYfv5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 1 and 2"
      ]
    },
    {
      "metadata": {
        "id": "rNuohp44N00i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several bandit algorithms.\n",
        "\n",
        "You will then run these algorithms on a multi-armed Bernoulli bandit problem, to understand the issue of balancing exploration and exploitation."
      ]
    },
    {
      "metadata": {
        "id": "ztQEQvnKh2t6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0tQ4aiAaIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "metadata": {
        "id": "YzYtxi8Wh5SJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NDhSYfSDcCC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set options"
      ]
    },
    {
      "metadata": {
        "id": "Ps5OnkPmDbMX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALrRR76eAd6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A generic multi-armed bandit class, with Bernoulli rewards"
      ]
    },
    {
      "metadata": {
        "id": "YP97bVN3NuG8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class BernoulliBandit(object):\n",
        "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "  \n",
        "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
        "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or tf.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A sampled reward according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    return reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cOu9RZY3AkF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "6EttQGJ1n5Zn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
        "  \"\"\"smoothing function for plotting\"\"\"\n",
        "  smoothed_array = []\n",
        "  value = initial_value\n",
        "  b = 1./smoothing_horizon\n",
        "  m = 1.\n",
        "  for x in array:\n",
        "    m *= 1. - b\n",
        "    lr = b/(1 - m)\n",
        "    value += lr*(x - value)\n",
        "    smoothed_array.append(value)\n",
        "  return np.array(smoothed_array)\n",
        "\n",
        "def one_hot(array, depth):\n",
        "  \"\"\"Multi-dimensional one-hot\"\"\"\n",
        "  a = np.array(array)\n",
        "  x = a.flatten()\n",
        "  b = np.eye(depth)[x, :depth]\n",
        "  return b.reshape(a.shape + (depth,))\n",
        "\n",
        "def plot(algs, plot_data, optimal_value, repetitions=30):\n",
        "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
        "  algs_per_row = 3\n",
        "  n_algs = len(algs)\n",
        "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
        "  fig = plt.figure(figsize=(15, 5*n_rows))\n",
        "  \n",
        "  for i, p in enumerate(plot_data):\n",
        "    for c in range(n_rows):\n",
        "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
        "      ax.grid(0)\n",
        "      ax.set_axis_bgcolor('white')\n",
        "\n",
        "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
        "      for alg in current_algs:\n",
        "        data = p.data[alg.name]\n",
        "        m = smooth(np.mean(data, axis=0))\n",
        "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
        "        if p.log_plot:\n",
        "          line = plt.semilogy(m, alpha=0.6, label=alg.name)[0]\n",
        "        else:\n",
        "          line = plt.plot(m, alpha=0.6, label=alg.name)[0]\n",
        "          plt.fill_between(range(number_of_steps), m + s, m - s,\n",
        "                           color=line.get_color(), alpha=0.2)\n",
        "      if not p.log_plot:\n",
        "        plt.plot([0, number_of_steps], [optimal_value]*2, '--k', label='optimal')\n",
        "\n",
        "      plt.ylim(p.ylim)\n",
        "      plt.title(p.title)\n",
        "      if i == len(plot_data) - 1:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "def run_experiment(bandit, algs, repetitions):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  action_dict = {}\n",
        "\n",
        "  for alg in algs:\n",
        "    reward_dict[alg.name] = []\n",
        "    action_dict[alg.name] = []\n",
        "\n",
        "    for _ in range(repetitions):\n",
        "      alg.reset()\n",
        "      reward_dict[alg.name].append([])\n",
        "      action_dict[alg.name].append([])\n",
        "      action = None\n",
        "      reward = None\n",
        "      for i in range(number_of_steps):\n",
        "        try:\n",
        "          action = alg.step(action, reward)\n",
        "        except:\n",
        "          print(alg, action, reward)\n",
        "          aoushd()\n",
        "        reward = bandit.step(action)\n",
        "        reward_dict[alg.name][-1].append(reward)\n",
        "        action_dict[alg.name][-1].append(action)\n",
        "        \n",
        "  return reward_dict, action_dict\n",
        "\n",
        "def train_agents(agents, number_of_arms, number_of_steps, repetitions=30,\n",
        "                 success_reward=1., fail_reward=0.):\n",
        "\n",
        "  success_probabilities = np.arange(0.25, 0.75 + 1e-6, 0.5/(number_of_arms - 1))\n",
        "  bandit = BernoulliBandit(success_probabilities, success_reward, fail_reward)\n",
        "\n",
        "  max_p = np.max(success_probabilities)\n",
        "  min_p = np.min(success_probabilities)\n",
        "  ylim_max = max_p*success_reward + (1 - max_p)*fail_reward\n",
        "  ylim_min = min_p*success_reward + (1 - min_p)*fail_reward\n",
        "  dif = ylim_max - ylim_min\n",
        "  ylim = (ylim_min - 0.1*dif, ylim_max + 0.1*dif)\n",
        "\n",
        "  max_prob = np.max(success_probabilities)\n",
        "  optimal_value = max_prob*success_reward + (1 - max_prob)*fail_reward\n",
        "\n",
        "  reward_dict, action_dict = run_experiment(bandit, agents, repetitions)\n",
        "  \n",
        "  smoothed_rewards = {}\n",
        "  expected_rewards = {}\n",
        "  regrets = {}\n",
        "  for agent, rewards in reward_dict.items():\n",
        "    smoothed_rewards[agent] = np.array(rewards)\n",
        "  for agent, actions in action_dict.items():\n",
        "    p_success = one_hot(actions, number_of_arms).dot(success_probabilities)\n",
        "    expected_rewards[agent] = p_success*success_reward + (1 - p_success)*fail_reward\n",
        "    regrets[agent] = optimal_value - expected_rewards[agent]\n",
        "\n",
        "  PlotData = namedtuple('PlotData', ['title', 'data', 'log_plot', 'ylim'])\n",
        "  plot_data = [\n",
        "      PlotData(title='Smoohted rewards', data=smoothed_rewards,\n",
        "               log_plot=False, ylim=ylim),\n",
        "      PlotData(title='Expected rewards', data=expected_rewards,\n",
        "               log_plot=False, ylim=ylim),\n",
        "      PlotData(title='Current Regret', data=regrets,\n",
        "               log_plot=True, ylim=(1e-2, 1)),\n",
        "      PlotData(title='Total Regret',\n",
        "               data=dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()]),\n",
        "               log_plot=False, ylim=(1e-0, 5e2)),\n",
        "  ]\n",
        "\n",
        "  plot(agents, plot_data, optimal_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HfE2fpb4A8rR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random agent"
      ]
    },
    {
      "metadata": {
        "id": "5EeGHFjslTeI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Random(object):\n",
        "  \"\"\"A random agent.\n",
        "  \n",
        "  This agent returns an action between 0 and 'number_of_arms', \n",
        "  uniformly at random. The 'previous_action' argument of 'step'\n",
        "  is ignored.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, number_of_arms):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = 'random'\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    return np.random.randint(self._number_of_arms)\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzpb_dGVjT0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A1: Implement agents\n",
        "\n",
        "Each agent, should implement the following methods:\n",
        "\n",
        "### `step(self, previous_action, reward)`:\n",
        "should update the statistics by updating the value for the previous_action towards the observed reward.\n",
        "\n",
        "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
        "\n",
        "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
        "\n",
        "### `reset(self)`:\n",
        "resets statistics (should be equivalent to constructing a new agent from scratch).  Make sure that the initial values (after a reset) are all zero.\n",
        "\n",
        "### `__init__(self)`:\n",
        "The `__init__` can be *the same* as for the random agent above (with the exception of $\\epsilon$-greedy---see below), except for the name, which should be unique (e.g., 'greedy', 'ucb', etc.)\n",
        "\n",
        "All agents should be in pure Python - so you cannot use TensorFlow to, e.g., compute gradients.  Using `numpy` is fine.\n"
      ]
    },
    {
      "metadata": {
        "id": "B8oKd0oyvNcH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 1: greedy\n",
        "**[10 pts]** You should **implement the greedy** agent, that\n",
        "\n",
        "1.   Estimates the average reward for each action that was selected so far, and\n",
        "2.   Always selects the highest-valued action.\n"
      ]
    },
    {
      "metadata": {
        "id": "Hyo1QCD4kePY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Greedy(object):\n",
        "\n",
        "  def __init__(self, number_of_arms):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = 'greedy'\n",
        "    self.reset()\n",
        "  \n",
        "  def step(self, previous_action, reward):\n",
        "    pass\n",
        "    \n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKfA7ifHvO-M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 2: $\\epsilon$-greedy\n",
        "**[15 pts]** You should **implement an $\\epsilon$-greedy** agent, that selects the highest-valued action with probability $1 - \\epsilon$, and otherwise selects an action at random.\n",
        "\n",
        "The exploration parameter $\\epsilon$ should be given in the `__init__`, as indicated in the code below.\n"
      ]
    },
    {
      "metadata": {
        "id": "-f8vILmXhqNF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy(object):\n",
        "\n",
        "  def __init__(self, number_of_arms, epsilon=0.1):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._epsilon = epsilon\n",
        "    self.name = 'epsilon-greedy epsilon:{}'.format(epsilon)\n",
        "    self.reset()\n",
        "  \n",
        "  def step(self, previous_action, reward):\n",
        "    pass\n",
        "  \n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDTyvlZsvSQq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 3: UCB\n",
        "**[15 pt]** You should **implement** an agent that **explores with UCB**. \n"
      ]
    },
    {
      "metadata": {
        "id": "0gnFb-urm_7C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class UCB(object):\n",
        "\n",
        "  def __init__(self, number_of_arms):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = 'ucb'\n",
        "    self.reset()\n",
        "  \n",
        "  def step(self, previous_action, reward):\n",
        "    pass\n",
        "    \n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmxL16v4vT4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 4: REINFORCE agents\n",
        "You should implement agents that implement REINFORCE policy-gradient methods\n",
        "\n",
        "The policy should be a softmax on action preferences:\n",
        "$$\\pi(a) = \\frac{\\exp(p(a))}{\\sum_b \\exp(p(b))}\\,.$$\n",
        "\n",
        "The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n"
      ]
    },
    {
      "metadata": {
        "id": "IAmMjutivZZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Assignment 4a:\n",
        "In the next text field, write down the update function to the preferences for all actions $\\{a_1, \\ldots, a_n\\}$ if you selected a specific action $A_t = a_i$ and received a reward of $R_t$.\n",
        "\n",
        "In other words, complete:\n",
        "\\begin{align*}\n",
        "p_{t+1}(a)\n",
        "& = \\ldots\n",
        "&& \\text{for $a = A_t$} \\\\\n",
        "p_{t+1}(b)\n",
        "& = \\ldots\n",
        "&& \\text{for all $b \\ne A_t$}\n",
        "\\end{align*}"
      ]
    },
    {
      "metadata": {
        "id": "KSP-7FOAMxdY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**[10 pts]** **Instructions**: please provide answer in markdown below.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TaSun8GrvZoF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Assignment 4b:\n",
        "**[15 + 5 pts]** You should **implement a vanilla REINFORCE agent with** and **without a baseline**.\n",
        "Whether or not a baseline is used should be a boolean constructor argument."
      ]
    },
    {
      "metadata": {
        "id": "2zMRtir2vW-S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class REINFORCE(object):\n",
        " \n",
        "  def __init__(self, number_of_arms, step_size=0.1, baseline=False):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._lr = step_size\n",
        "    self.name = 'reinforce, baseline: {}'.format(baseline)\n",
        "    self._baseline = baseline\n",
        "    self.reset()\n",
        "  \n",
        "  def step(self, previous_action, reward):\n",
        "    pass  \n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jZsPzCmDxAh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 5: Analyse Results"
      ]
    },
    {
      "metadata": {
        "id": "xQkk8sMxE0N4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the cell below to train the agents and generate the plots for the first experiment.\n",
        "\n",
        "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 1, and a reward on failure of 0."
      ]
    },
    {
      "metadata": {
        "id": "GsNBHNZtHCPe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "height": 554
        },
        "cellView": "both",
        "outputId": "e318345d-2d71-4b51-ce09-f7876be95346",
        "executionInfo": {
          "status": "error",
          "timestamp": 1517268340956,
          "user_tz": 0,
          "elapsed": 812,
          "user": {
            "displayName": "Diana Borsa",
            "photoUrl": "//lh3.googleusercontent.com/-ncAFLYhkpa4/AAAAAAAAAAI/AAAAAAAABCo/oObvci_cgWA/s50-c-k-no/photo.jpg",
            "userId": "113702420363391077417"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment 1: Bernoulli bandit\n",
        "\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "agents = [\n",
        "    Random(number_of_arms),\n",
        "    Greedy(number_of_arms),\n",
        "    EpsilonGreedy(number_of_arms, 0.1),\n",
        "    EpsilonGreedy(number_of_arms, 0.01),\n",
        "    UCB(number_of_arms),\n",
        "    REINFORCE(number_of_arms),\n",
        "    REINFORCE(number_of_arms, baseline=True),\n",
        "]\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-a82cefc8112a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain_agents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_arms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-4-0c78eb9f077c>\u001b[0m in \u001b[0;36mtrain_agents\u001b[1;34m(agents, number_of_arms, number_of_steps, repetitions, success_reward, fail_reward)\u001b[0m\n\u001b[0;32m     94\u001b[0m   \u001b[0moptimal_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_prob\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msuccess_reward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmax_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfail_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[0mreward_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m   \u001b[0msmoothed_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-4-0c78eb9f077c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(bandit, algs, repetitions)\u001b[0m\n\u001b[0;32m     72\u001b[0m           \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m           \u001b[0maoushd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mreward_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0maction_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-3-90e1f9d198f7>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_of_arms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m       raise ValueError('Action {} is out of bounds for a '\n\u001b[1;32m---> 32\u001b[1;33m                        '{}-armed bandit'.format(action, self._number_of_arms))\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Action None is out of bounds for a 5-armed bandit"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MWyaMm-sjz9a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment 5 a.\n",
        "(Answer inline in the markdown below each question.)\n",
        "\n",
        "**[5pts]** Name the best and worst algorithms, and explain (with one or two sentences each) why these are best and worst.\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[5pts]** Which algorithms are guaranteed to have linear total regret?\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[5pts]** Which algorithms are guaranteed to have logarithmic total regret?\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[5pts]** Which of the $\\epsilon$-greedy algorithms performs best?  Which should perform best in the long run?\n",
        "\n",
        "*Answer here.*"
      ]
    },
    {
      "metadata": {
        "id": "0YO5NDaPGDsp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the cell below to train the agents and generate the plots for the second experiment.\n",
        "\n",
        "Trains the agents on a bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 0, and a reward on failure of -1."
      ]
    },
    {
      "metadata": {
        "id": "7cvJf4WzmJXK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment 2: R = 0 on success, R = -1 on failure.\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             success_reward=0., fail_reward=-1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GOe5RDsnj4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment 5 b.\n",
        "(Answer inline in the markdown.)\n",
        "\n",
        "**[10pts]** Explain which algorithms improved from the changed rewards, and why.\n",
        "\n",
        "(Use at most two sentences per algorithm and feel free to combine explanations for different algorithms where possible).\n",
        "\n",
        "*Answer here*"
      ]
    }
  ]
}