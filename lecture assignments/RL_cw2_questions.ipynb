{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Assignment_2 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL homework 2\n",
        "**Due date: 26 February 2018, 23:55am **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sns0IKYNtsA"
      },
      "source": [
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<student_id>_ucldm_rl2.ipynb`** before the deadline above.\n",
        "\n",
        "Do not forget to include the PDF version on the .zip submitted in Moodle.\n",
        "\n",
        "Also send a sharable link to the notebook at the following email: **`ucl.coursework.submit@gmail.com`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB0tQ4aiAaIu"
      },
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDhSYfSDcCC"
      },
      "source": [
        "### Set options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "source": [
        "### A grid world"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "source": [
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1, -1,  0, 10,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "    ])\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\", cmap='pink')\n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(8, 3, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
        "    \n",
        "    self._state = new_state\n",
        "\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(action_values, vmin=0, vmax=10):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "  \n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "  \n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# Part 1: Implement agents\n",
        "\n",
        "Each agent, should implement a step function:\n",
        "\n",
        "### `step(self, reward, discount, next_observation, ...)`:\n",
        "where `...` indicates there could be other inputs (discussed below).  The step should update the internal values, and return a new action to take.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" (for whatever definition of $v$ is appropriate) in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get the initial observation by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`.\n",
        "\n",
        "In this assignment, observations will be states in the environment, so the agent state, environment state, and observation will overlap, and we will use the word `state` interchangably with `observation`.\n",
        "\n",
        "All agents should be in pure Python - so you cannot use TensorFlow to, e.g., compute gradients.  Using `numpy` is fine.\n",
        "\n",
        "### A note on the initial action\n",
        "Normally, you would also have to implement a method that gives the initial action, based on the initial state.  In our experiments the helper functions above will just use the action `0` (which corresponds to `up`) as initial action, so that otherwise we do not have to worry about this.  Note that this initial action is only executed once, and the beginning of the first episode---not at the beginning of each episode.\n",
        "\n",
        "Some algorithms (Q-learning, Sarsa) need to remember the last action in order to update its value when they see the next state.  In the `__init__`, make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._last_action = 0\n",
        "  (...)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGeLcsvixmt"
      },
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFuWFzIi5uB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "641cbe88-3c7f-424f-9bdf-28034b1fe0c9"
      },
      "source": [
        "grid = Grid()\n",
        "grid.plot_grid()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2N2gd11Qctt"
      },
      "source": [
        "## Random agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkbzl48jQcFn"
      },
      "source": [
        "# For reference: here is a random agent\n",
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(number_of_actions)\n",
        "    return next_action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "source": [
        "\n",
        "## Agent 1: TD learning\n",
        "**[5 pts]** Implement an agent that behaves randomly, but that _on-policy_ estimates state values $v(s)$, using one-step TD learning with a step size $\\alpha=0.1$.\n",
        "\n",
        "Also implement `get_values(self)` that returns the vector of all state values (one value per state).\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement `get_values` and `step`.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.  Hint: in the `step` you similarly will want to store the previous state to be able to compute the next TD error on the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyo1QCD4kePY"
      },
      "source": [
        "# State value function evaluation. Update rule: \n",
        "# V(s) <- V(s) + step_size*(r + V(s+1) - V(s))\n",
        "\n",
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self._values = np.zeros(number_of_states)\n",
        "    self._state = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    \n",
        "  def get_values(self):\n",
        "    return self._values\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    \"\"\"\n",
        "    Params: \n",
        "    r (int): reward value sampled from the environment. Used for the update\n",
        "    g (float): discount factor\n",
        "    s (int): next state (t+1) sampled. Bootstrap from s\n",
        "    Returns:\n",
        "    next_action (int): refer to Grid() class for action encoding\n",
        "    \"\"\"\n",
        "    # Value function update\n",
        "    self._values[self._state] += self._step_size * (r + g*self._values[s] - self._values[self._state])\n",
        "\n",
        "    # Current state update\n",
        "    self._state = s\n",
        "\n",
        "    # Randomly pick action from policy (random uniform)\n",
        "    next_action = np.random.choice([i for i in range(self._number_of_actions)],\n",
        "                                   p = [1/self._number_of_actions for _ in range(self._number_of_actions)])\n",
        "    \n",
        "    return next_action"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaMmp1lDgpUG"
      },
      "source": [
        "Run the next cell to run the `RandomTD` agent on a grid world."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ZoYwgZfho2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "71b6f500-2638-4581-a435-58efeebb7501"
      },
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "agent = RandomTD(grid._layout.size, 4, grid.get_obs())\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "v = agent.get_values()\n",
        "plot_values(v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxc_Sx7og4JH"
      },
      "source": [
        "If everything worked as expected, the plot above will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero (shown in orange above).\n",
        "\n",
        "### Policy iteration\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state the policy would for each action look at the value of the resulting state, and would then pick the action with the highest state value. You do **not** have to implement this, just answer the following questions.\n",
        "\n",
        "**[5 pts]** Would the greedy after one such iteration of policy evaluation and policy improvement be optimal on this problem?  Explain (in one or two sentences) why or why not.  \n",
        "*No, the resulting greedy policy would not be optimal, since it is greedy w.r.t. a non optimal value function*\n",
        "\n",
        "**[5 pts]** If we repeat the process over and over again, and repeatedly evaluate the greedy policy and then perform another improvement step, would then the policy eventually become optimal?  Explain (in one or two sentences) why of why not.  \n",
        "*No, since the Greedy policy does not satisfy the 1st of the 2 GLIE condition. Namely, greedy policy does not guarantee infinite exploration of states asymptotically*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfA7ifHvO-M"
      },
      "source": [
        "\n",
        "## Agent 2: Q-learning\n",
        "**[10 pts]** Implement an agent that uses **Q-learning** to learn action values.  In addition, the agent should act according to an $\\epsilon$-greedy policy with respect to its action values.\n",
        "\n",
        "**[10 pts]** Include an option to use **Double Q-learning**, with a `double` boolean flag in the **__init__**.  If `double=False` the agent should perform Q-learning.  If `double=True` the agent should perform Double Q-learning.  Note that we then need two action-value functions.\n",
        "\n",
        "**[10 pts]** Include an option to use **Sarsa** instead of Q-learning, in the **step**.\n",
        "\n",
        "**[15 pts]** Generalize to **General Q-learning**, where the `__init__` takes a functions `target_policy` and `behaviour_policy`.  The function `behaviour_policy(action_values)` should map `action_values` to a single action.  For instance, the random policy can be implemented as:\n",
        "```\n",
        "def behaviour_policy(action_values):\n",
        "  return np.random.randint(len(action_values))\n",
        "```\n",
        "We will typically just use $\\epsilon$-greedy, for instance:\n",
        "```\n",
        "def behaviour_policy(action_values):\n",
        "  return epsilon_greedy(action_values, epsilon=0.1)\n",
        "```\n",
        "\n",
        "The target policy is defined by a function `target_policy(action_values, action)`, which should return **a vector** with one probability per action.  The `action` argument is used to be able to do Sarsa: in addition to the action values, the function will also get the action as selected by the behaviour so that it can return a one hot vector for just the selected action in the Sarsa case.  So, the target policy for Sarsa would look like this:\n",
        "```\n",
        "def one_hot(index, max_index):\n",
        "  np.eye(max_index)[index]\n",
        "\n",
        "def target_policy(action_values, action):\n",
        "  return one_hot(action)\n",
        "```\n",
        "As another example, a random target policy is:\n",
        "```\n",
        "def target_policy(action_values, unused_action):\n",
        "  number_of_actions = len(action_values)\n",
        "  return np.ones((number_of_actions,))/number_of_actions\n",
        "```\n",
        "\n",
        "Note that **double learning** can be combined with General Q-learning, but is separate.  So, the `double` flag in the init remains.  E.g., when the target policy is the Sarsa policy described above and `double=True`, the algorithm should implement **double Sarsa**.\n",
        "\n",
        "Note also that if (or when) you have implemented General Q-learning, this algorithm encompasses all previous algorithms, so you only need this one algorithm with, as its interface the two functions\n",
        "\n",
        "`__init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1)`\n",
        "\n",
        "and\n",
        "\n",
        "`step(self, reward, discount, next_state)`\n",
        "\n",
        "We will mostly use `step_size=0.1`, so make that the default, but allow it to change when it is fed in as an argument.\n",
        "\n",
        "If you do not success in implementing General Q-learning, try to implement at least Sarsa, Q-learning, and Double Q-learning, to be able to answer questions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq_qf3E_Z7NT"
      },
      "source": [
        "class GeneralQ(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    self._s = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    self._last_action = 0\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    if self._double:\n",
        "      return (self._q + self._q2)/2\n",
        "    else:\n",
        "      return self._q\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    \"\"\"TODO Implement\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "# Part 2: Analyse Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "### Run the cells below to train a Q-learning and a SARSA agent and generate plots.\n",
        "\n",
        "This trains the agents the Grid problem with a step size of 0.1 and an epsilon of 0.1.\n",
        "\n",
        "The plots below will show action values for each of the actions, as well as a state value defined by $v(s) = \\sum_a \\pi(a|s) q(s, a)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsNBHNZtHCPe",
        "cellView": "both"
      },
      "source": [
        "grid = Grid()\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, 0.1)\n",
        "agent = GeneralQ(grid._layout.size, 4, grid.get_obs(),\n",
        "                 target_policy, behaviour_policy, double=False)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DwHqxeZ1rXa"
      },
      "source": [
        "grid = Grid()\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[a]\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, 0.1)\n",
        "agent = GeneralQ(grid._layout.size, 4, grid.get_obs(),\n",
        "                 target_policy, behaviour_policy, double=False)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGptHwE23lmP"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Consider the greedy policy with respect to the estimated values\n",
        "\n",
        "**[10 pts]** *How* do the policies found by Q-learning and Sarsa differ?  (Explain qualitatively how the behaviour differs in one or two sentences.)\n",
        "\n",
        "**[10 pts]** *Why* do the policies differ in this way?\n",
        "\n",
        "**[10 pts]** Which greedy policy is better, in terms of actual value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoZD09jt6ow0"
      },
      "source": [
        "### Noisy environments\n",
        "\n",
        "We will now compare Q-learning and Double Q-learning on a noisy version of the environment.\n",
        "\n",
        "In the noisy version, a zero-mean Gaussian is added to all rewards.  The variance of this noise is higher the further to the left you go, and the further down (so away from the goal).\n",
        "\n",
        "Run the cell below to run 20 repetitions of the experiment that runs Q-learning and Double Q-learning on this noisy domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LKhFNDF3jCJ"
      },
      "source": [
        "def target_policy(q, a):\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, 0.1)\n",
        "mean_reward_q_learning = []\n",
        "mean_reward_double_q_learning = []\n",
        "for _ in range(20):\n",
        "  grid = Grid(noisy=True)\n",
        "  q_agent = GeneralQ(grid._layout.size, 4, grid.get_obs(),\n",
        "                     target_policy, behaviour_policy, double=False, step_size=0.1)\n",
        "  dq_agent = GeneralQ(grid._layout.size, 4, grid.get_obs(),\n",
        "                      target_policy, behaviour_policy, double=True, step_size=0.1)\n",
        "  mean_reward_q_learning.append(run_experiment(grid, q_agent, int(2e5)))\n",
        "  mean_reward_double_q_learning.append(run_experiment(grid, dq_agent, int(2e5)))\n",
        "plt.violinplot([mean_reward_q_learning, mean_reward_double_q_learning])\n",
        "plt.xticks([1, 2], [\"Q-learning\", \"Double Q-learning\"], rotation=60, size=12)\n",
        "plt.ylabel(\"average reward during learning\", size=12)\n",
        "ax = plt.gca()\n",
        "ax.set_axis_bgcolor('white')\n",
        "ax.grid(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oC71jLmpRkq"
      },
      "source": [
        "q = q_agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q, vmin=-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4W2uqjapUNg"
      },
      "source": [
        "q = dq_agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q, vmin=-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itgvt-pQpKFD"
      },
      "source": [
        "The plots above show 1) the distributions of average rewards (over all learning steps) over the 20 experiments per algorithm, 2) the action values for Q-learning, and 3) the action values for Double Q-learning.\n",
        "\n",
        "**[10 pts]** Explain why Double Q-learning has a higher average reward. Use at most four sentences, and discuss at least a) the dynamics of the algorithm, b) how this affects behaviour, and c) why the resulting behaviour yields higher rewards for Double Q-learning than for Q-learning.\n",
        "\n",
        "(Feel free to experiment to gain more intuition, if this is helpful.  Especially the action value plots can be quite noisy, and therefore hard to interpret.)"
      ]
    }
  ]
}