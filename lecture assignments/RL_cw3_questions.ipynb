{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL3 - Q",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL homework 3\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "\n",
        "**Name:** Your Name\n",
        "\n",
        "**SN:** Your Student Number\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Start date:** *7th March 2018*\n",
        "\n",
        "**Due date:** *21st March 2018, 11:55 pm*\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_RL_hw3.ipynb** before the deadline above.\n",
        "\n",
        "Also send a **sharable link** to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.\n",
        "\n",
        "Please compile all results and all answers to the understanding questions into a PDF. Name convention: **studentnumber_RL_hw3.pdf**. Do not include any of the code (we will use the notebook for that). \n",
        "\n",
        "**Page limit: 10 pg **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will investigate the properties of 3 distinct reinforcement learning algorithms:\n",
        "\n",
        "* Online Q-learning\n",
        "* Experience Replay\n",
        "* Dyna-Q\n",
        "\n",
        "We will consider two different dimensions:\n",
        "* Tabular vs Function Approximation\n",
        "* Stationary vs Non-Stationary environments\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms **[50 pts]**.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties.\n",
        "\n",
        "Finally you will answer a few question about the performance of these algorithms in the various problems **[50pts]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "source": [
        "# Grid worlds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMC6nODK1HAV"
      },
      "source": [
        "**Tabular Grid-World**\n",
        "\n",
        "Simple tabular grid world.\n",
        "\n",
        "You can visualize the grid worlds we will train our agents on, by running the cells below.\n",
        "`S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.\n",
        "\n",
        "We will use three distinct GridWorlds:\n",
        "* `Grid` tabular grid world with a goal in the top right of the grid\n",
        "* `AltGrid` tabular grid world withh a goal in the bottom left of the grid\n",
        "* `FeatureGrid` a grid world with a non tabular representation of states, the features are such to allow some degree of state aliasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP97bVN3NuG8",
        "cellView": "form"
      },
      "source": [
        "#@title Grid\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, discount=0.9):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "    ])\n",
        "    self._start_state = (2, 2)\n",
        "    self._goal_state = (8, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._discount = discount\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\")     \n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(\n",
        "        self._start_state[0], self._start_state[1], \n",
        "        r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(\n",
        "        self._goal_state[0], self._goal_state[1], \n",
        "        r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "  \n",
        "  def int_to_state(self, int_obs):\n",
        "    x = int_obs % self._layout.shape[1]\n",
        "    y = int_obs // self._layout.shape[1]\n",
        "    return y, x\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = self._discount\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = self._discount\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXyPvOq-S2OT",
        "cellView": "form"
      },
      "source": [
        "#@title AltGrid\n",
        "class AltGrid(Grid):\n",
        "  \n",
        "    def __init__(self, discount=0.9):\n",
        "      # -1: wall\n",
        "      # 0: empty, episode continues\n",
        "      # other: number indicates reward, episode will terminate\n",
        "      self._layout = np.array([\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0, 10,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "      ])\n",
        "      self._start_state = (2, 2)\n",
        "      self._goal_state = (2, 7)\n",
        "      self._state = self._start_state\n",
        "      self._number_of_states = np.prod(np.shape(self._layout))\n",
        "      self._discount = discount"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1OFB5Mv1gC_",
        "cellView": "form"
      },
      "source": [
        "#@title FeatureGrid\n",
        "class FeatureGrid(Grid):\n",
        "  \n",
        "  def get_obs(self):\n",
        "    return self.state_to_features(self._state)\n",
        "  \n",
        "  def state_to_features(self, state):\n",
        "    y, x = state\n",
        "    x /= float(self._layout.shape[1] - 1)\n",
        "    y /= float(self._layout.shape[0] - 1)\n",
        "    markers = np.arange(0.1, 1.0, 0.1)\n",
        "    features = np.array([np.exp(-40*((x - m)**2+(y - n)**2))\n",
        "                         for m in markers\n",
        "                         for n in markers] + [1.])\n",
        "    return features / np.sum(features**2)\n",
        "  \n",
        "  def int_to_features(self, int_state):\n",
        "    return self.state_to_features(self.int_to_state(int_state))\n",
        "  \n",
        "  @property\n",
        "  def number_of_features(self):\n",
        "      return len(self.get_obs())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUhh2qqwep_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "58a5f0e1-ba92-46be-b675-eb018a8d9ec1"
      },
      "source": [
        "# Instantiate the two tabular environments\n",
        "grid = Grid()\n",
        "alt_grid = AltGrid()\n",
        "\n",
        "# Plot tabular environments\n",
        "grid.plot_grid()\n",
        "alt_grid.plot_grid()\n",
        "\n",
        "# Instantiate the non tabular version of the environment.\n",
        "feat_grid = FeatureGrid()\n",
        "\n",
        "# Plot the features of each state\n",
        "shape = feat_grid._layout.shape\n",
        "f, axes = plt.subplots(shape[0], shape[1])\n",
        "for state_idx, ax in enumerate(axes.flatten()):\n",
        "  ax.imshow(np.reshape((feat_grid.int_to_features(state_idx)[:-1]),(9,9)), interpolation='nearest')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcYAAAE8CAYAAABaaxFWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5Bc1X0n8O+93T3T835pRiNppNEDCfyQwd41gsUvjB2WWMmyjk055QInNrHD1qa27Kyx/3CSKuJKYti1K6lUMNg4MS4qLowJG7Adx1KMwRhLToxAIEBC0ow0kub9fvT049794/ac8ztX0z3d6u7bt0ffzz+c6T4985tLt87c3znndyzXdUFEREQeu9oBEBERhQkHRiIiIoEDIxERkcCBkYiISODASEREJESL6byhM+Ju3xoDALx6rlt/k+klo5+bccoQGpDAApLuslXK96jFmCMtTW60qwMAUD8uVg0vJsyOZVxRPIepcdd1u9fumVtdtNFtiLV5XyRT6nHXKc+19SvHta6z6t04msoV0prKETNQm3Ez5sKU5bPIa72mfDEXNTBu3xrD4R9vBQDs++Jd6vGuJ14x+mVmZ4sOcjWH3IMlf49ajDna1YHeP/sjAMDub+oBxv71a0Y/d3m55J+14oD72GCp36Mh1obrd/4+AMAdGlaPO/PzZscyDejluNZxNGFf5De8L5xMyd9vLeWIGcjGbd1Ulu9ViLJda8a8pnJ8Fnmt15YvZqZSiYiIhKLuGF89163uujY8c049nllcLG9UZXRyuQW3nfL+CmkcTavH3XQ610uqbm/bOA7f8k0AwL6fibvcV+qNfpky3jGWQ6I7itfu6gQA7HlYx2odPWH0K+edbqms+jpE+7yMgjM8qh53lsxUeznT1mVjZbNAYYyNqIbxjpGIiEjgwEhERCQUlUqNTi+pRSsyfRrmtGR6uB4jX9kFAGh+8Yx+PBGedJ7fuXQDvjS6FwAQn9ILQtxM5ReHlGJvxxgOf+QBAMC+fxcp4OPhTQEvb4jhxKc3AQB2fbdRPW698obRz00lA41rLVYshmjvZgCAMz6hHncSiVwvCQemf6kG8I6RiIhI4MBIREQkcGAkIiISippjdDNO2TbCB8WeS6Dx6VcBAOkFsa0kgM3cl2p2uBkH770BAND5ktgWE6K5udVMOzaeWGgGAMQWdbWbMJ/5+dauMRy+434AwL5jYl70VNzolwnZHGOyqw4Dt/cDAPr/n64WYp0YMPqFaW7UikYR6dwAAHCmZ9TjYYqRCOAdIxERkYEDIxERkVBUKrUWuY4DZ26u2mEUJTqTQMcPvfRvZn5BPR7mbTEAcH6kC/d89XYAQO8RUSvVX0UmRJJuBkNpr5arndYp3zCnfwFgd/cwnrrrXgDARwf/t3q8/ewFo1+Y0pSpjnoMf+QKAEDvT8T7Y+Cs0S9M73MrEkGk1SuM79TQZ5FKwztGIiIigQMjERGRsO5TqbXIzWSQEav2akVsahm9j3kVY4xVhyFOO50Y68XN938OANB/ZFw97oS4MhIA1FkR9EW9FcBOVB8pZ1klH/VYMZs3TuBPP/cdAMD/Gfu4erxlZNzo54Zo6iPdFsfULW8CAHQ+J1aIn/OlrEP2HrdsG3ZzCwDAqZHV+GHCO0YiIiKBAyMREZHAVCqVjZtOIzOeTYuFfFXnirqJJLZ/xzswXRbjDtNqztW8PNGNPQ97BQl2vaTT1s5SeIuIt9sObm3yVgD/ZaP+mzzM6d/W3nncdPdzAIBn/+R69Xjj+KTRL2ypVKcljsX3eing5n8XhyeMjPk6hie1akVsRJpbAQBOlQ+p4B0jERGRwIGRiIhI4MBIREQkcI6RyqtG5hZXuKkU0ufOZ7+ondjrx1PY/aC3ZcAZHlWPu+lUtUJa09Gpbux87DMAgD3H9JYMJ8TF8bdEl/DlnqMAgH0d71KPN0Ui1QqpINHeZWz8wkkAwNiXd6rH48+Yh0DIubxqS7c3YGL/WwAAG54RW2OGzhv9gphz5B0jERGRwIGRiIhIYCqVqIZSqCvc5STSK8W3Q7TkPp/4WBpX3e9tc3CHRBHxZHi3xhyd2YAdP7oTALD7hE47hjn9CwC76ufw6M6DAIB9PXvU4w3R8P6T/6YtYzj0V9mzUb8ozkZ9YtroF8SZwLxjJCIiEjgwEhERCVYx585ZljUGYLBy4Vyk33Xd7lK+AWMuWC3GfVnGDNRm3Iy5YLUY97qKuaiBkYiIaL1jKpWIiEjgwEhERCRwYCQiIhKK2tSyoTPibt8aAwAMp+vV41MjreY3ndZH37iZS99jlcACku5ySWfS1GLMkZYmN9rVAQCw0vpb1U07Rj9rSe+lch3zuWLNYWq81Mnzumij2xBr875wxNy1r4RTqbGuKMe1rrPq3TiayhJPIcoRM1CbcTPmwpTls8hrvaZ8MRc1MG7fGsPhH28FAHxlYrd6/PH7PmD063ryNdXOTE0V8yMMh9yDl/zaFbUYc7SrA71/9kcAgLrhmHq8/ymzrmHkxROqXWrNwwPuYyWvBmuIteH6nb8PALASetO24zu7zpmf11+UsPirHNc6jibsi/xGNhYxYFdoUVo5YgaycVs3leV7FaJs15oxr6kcn0Ve67Xli7mogfHVc92qIkH9rL6r2nDknNEvMzeHsDi53ILbTnkX+4UzW9XjfaO+u5gQHTS6t20ch2/5JgDfYP66bzB/Td8BIwTFgBPdUbx2VycAID6iiyxv+3GL0c9++Q3VdhLVPVjXqq9DtC/7vkjqAtzOpPnHkbO0pL8Iy0ru1Q74DUtsRDWMc4xEREQCB0YiIiKhqFRqdHoJXU+8AsBcoJLxFdQNU1oyPVyPka/sAgD0z+u46l87a/TLyFRZlZ1LN+BLo3sBAD86+yb1ePOUb1FQCYuEKmFvxxgOf+QBAL4U8FlfCvhkg/6iyqnU5Q0xnPj0JgBAfEynJrccbDP6Wa+eVG03BAWkrVgM0d7NALwzJVe4M77z9mSsYUizMv1LNYB3jERERAIHRiIiIoEDIxERkVDUHKObcQI5JLKc7LkEGp9+1ftCzMmlE755ohAd9jo73IyD994AAGie1Xvrmo9eMPplQrBFQ5p2bDyx0AwAeG5yl3o8tujb0O+WZ4N/Oby1awyH7/AORzXmRcd886KDjartn1OvhmRXHQZu7wcAxCf1HN3GZyeMftZJvSWu2nOjVjSKSOcG7wuxDsGZXzD6uanwHlxMlwfeMRIREQkcGImIiISiUqm1yHUcOCGqxFOI6EwCHT/00r9uRqcd/VtKwrQtBgDOj3Thnq/eDgCon9bpvY4jI0Y/ZyE8W2OSbgZDaa9E3WhSV+ix0+HeQrC7exhP3XUvAOBrY+9Rj/9i7lqjX8eFMdWudgo41VGP4Y9cAcD3/viV7/0xqCtpVTutakUiiLR6W3fkZ9EN+WeRSsM7RiIiIoEDIxERkbDuU6m1yM1kkJmeqXYYRYtNLaP3sWyB8BpZdXhirBc33/85AL7VnUfM1Z2lnl5SbnVWBH1RbwVwT52eKnCiJZ9oVTGbN07gTz/3HQDAt86/Sz0+meo3+rWK01gy09V9r6Tb4pi6xas+VZdvhfg5/XUY0qqWbcNuzk4NiNX4TohX44cJ7xiJiIgEDoxEREQCU6lUNm46jcz4+CpPhHeFZ91EEtu/422Cz1eM202GJ/0LAC9PdGPPw97ZqEbx85emjX5hSgG32w5ubfJWAL/eqYuyP964w+xohefv9dbeedx093MAzIL+eHCT0a9pSk99hKEIitMSx+J7vXhjxuEJ541+mTH9ea12CtiK2Ig0t3qxiPSvvzBFEHGG5x1IREQUAhwYiYiIBA6MREREAucYqbxCPJ+4GjeVQvrc+VWeCPfvUT+ewu4Hs1sEknpu1JmcMvqFaW706FQ3dj72GQBAfCSiHt92zDefG6JDw7dEl/DlnqMAgJaIPlT78Q6zyHxTJIIwifYuY+MXvHncF85sVY/3PbzV6Bd/Tm+lcqtcISzd3oCJ/W8BANTP6jnGliPDRr/MkP68Vmq+kXeMREREAgdGIiIigalUopCnTVfjLieRHjib/UKcbxni3yU+lsZV93tVbayETvE6otINADghOO9yxdGZDdjxozsBAHXDMfV4/wlzG0y1z7r021U/h0d3HgQAfKVNnDPaY6aAG6LhGQLetGUMh/5qlbNR7/OdjfqkTvlmpsypg3LhHSMREZHAgZGIiEiw3CJSL5ZljQEYrFw4F+l3Xbe7lG/AmAtWi3FfljEDtRk3Yy5YLca9rmIuamAkIiJa75hKJSIiEjgwEhERCRwYiYiIBA6MREREQlG7OyNNTW6svRMA8NYNYzn7vTyuF/rUj4sz7oqs25jAApLusrV2z9xqMeZYfZNb3+jFnGrLvTgqNqN/TGRe13F005nVuuc1h6nxUleV1dlxtyHS4n2Rb+OwqG94KbGuKMe1rrPq3TiaSvkWRSlHzEBtxs2YC1OWzyKv9ZryxVzUwBhr78TW//FZAMDhT92fs9+VD92l2lc8dE6102d0G87a/yAecg8WE96qajHm+sZOXP3+/wUAGNqf+2f2PaULF7c8c0K1M7KQdIGrjg+4j5W8TLoh0oLr2z/sfdHTlbvj6IRqZsQBr4VcX6kc1zqOJuyzs5U18h2QW6bqMuWIGcjGbd1Ulu9ViLJda8a8pnJ8Fnmt15Yv5qIGxvqpDHZ+z/tHd9+Ju3L223lE/8PsDI/qJ4r8h68c7CTQcsZr33Yq90Vf6QMASIjyTvIfxICkG4HRd3j/SJ++5YGc/a48r/8ftL7QrJ+Qg40b3DV3muqxeN0uAMCZm3MPMtt+3K7ajc8dV+3MjDhlIaBtRFYkgkird2q41dmRs58r/tjIzM7rJ6rwnr6IlecPdW7HIioa5xiJiIgEDoxERERCUalUN7EM99VTAICugXjOfs6SWAiSqu5BqbG5FHqeHgEAjAzvytmv57UR1c5MTesnqpCKstNAw6iXHvvS6N6c/Vb6AACWxXWuQvoXAFJNFi5c572lTn0kz3zujJjPfaVNPyFTlAGlgJ2meiSu9Sr5D70/lrNf379tUO34L0X6d3Z2te4VZ9k27GZvoZPd1pqznyPS0868vL5VTrEy/UshxjtGIiIigQMjERGRwIGRiIhIKO74ZtdVc4aZKs8dFspNpuAMDAEAGodzb/B3xBaN6s+LZrDpp96p5gcnbsjZb9NL+uRzp8rzogBgZYC6WW/u6ImF5pz9VvoAAOQG/yrMjaaaLZx7tze3ePyOPPOiKTEvelxvN8H8gm4HuHXDbaxH+porAADn39WQs9/mny+pdvTIG6rtzM2t1r2iLMuCHffWJlgtLTn7uSI2Z1luneLcIwWDd4xEREQCB0YiIiKhuFRqLRLp32qnSAvlJpbhnjgNAOgYKnBbTJE1XSshNpfBlp962wPumb89Z78tvxJbCC6hfF1ZuYCd8lK7Q+n5nN1W+nivqX5KL9liY+hGL4X6kzvvzdnvg7G7VXvnGZ0CdhYWdaeAUsBuQz3ct3jp35F35k6l9vxKp1LtY6dU21lcXK17ZVkWrPp6L5aGS/gshuC9QsXjHSMREZHAgZGIiEhY/6nUWuS6cLOr8TJyVV7IWUvLsF72Vj72Dl7CqsMqqJtz0PdTb+Xmzam7c/brE6s7nUmxArhaRcQtwIl5abq+aO4VwCt9vNeUfNpVSVItEZy70avS8+effjhnvz958A7V3jasC7s7S/r/QWBF5uP1sK7YAQCYelt7zn7tL+n3hPXGgGo7icQqvQNgWbBidV4zXp+zmxui1fhhwjtGIiIigQMjERGRwFQqlY3rujp1lC9FGqKVetbistr4vv30JRTjrpLYvIstz6YAAHtiuc9G7cv2AQC3yilgNwIkW73/97c25b6GX2gV74+oPozbOEg6oCLzqZYILtzYCQDY/6lnc/Z76qF3q/aWCX2dnWF9OEGQ73urLga7vw8AsHDVhpz9ml4bV21HHMruVmOKQ6R/C14BXKH0L+8YiYiIBA6MREREAgdGIiIigXOMVBkhmkfMx3UcVVA779xhyH4fe2EZ8cMnAAC73+jI2c8VlYUysuB5FcQWXGz6ZRoAsLPtMzn7bcv2AQB3ekY/UYUi804UWOrx/t9/uedozn7f63mX/qK+TrerMC8KAKmWGEbftxEAsPMTx3P2O/XtPaq98Un9/kiPjOpOQW6N2bMTADBxTe73dOcR/Z62jg+odjm3xvCOkYiISODASEREJDCVSrQiZOnSfNxMBpmVLSSz+VLAIv1Y5d/PXlhG4y9PAgCuOtWVu+PohGpm5O9Whfiji0DPr71ruGPznTn79f1aX2d3TsYcfPoXAJw6YG6b135058Gc/a7cJlKpskJOFVLAyx0RnPqol0J9/VN5zkZ9SJyN+pAuLC+3m5S6HYl3jERERAIHRiIiIsFyi0hPWJY1BmCwcuFcpN913e5SvgFjLlgtxn1ZxgzUZtyMuWC1GPe6irmogZGIiGi9YyqViIhI4MBIREQkcGAkIiISODASEREJRW3wr7PjboPdAgBIt+vzsjo2zhr9eqP6LK+jM/ossPio2ATrr2u3yhqgBBaQdJetYmJcFzFbcbfBbvZ+RIPedJtsN/+OcaM6gOi8/pHRWX1GmZtKoRBzmBovdVVZnVXvxtEEALBsEWvU9zazxeVJ6424bkZsyi1gUVh5rrWOOQjliBmozbgZc2HK/VkMQi1e63wxFzUwNtgtuL7tvwMAJvZfpR7/8OcPGP2+0HVCtXc89QeqfdXfzqm2e+wN4zVuOg2/Q27uig2Fqs2Ym3Fdw4cAAJmrd6vHB/c3Gv2SvXrQ634mptsH9Irn9IUR4zW5KkIccB8reZl0HE3YZ38AAGA3N6vH7Q2dRj83LoosT+oi0Y44SLeQA0jLca3jaMI+66aLn7B8nxejEsilV5MpR8xAnrgrpKLXukJqMWagjJ9FXuu88sVc1MDoOhl1EsGGZ3T5nR/M3mj0e7z1A6q957iu2G4NnFdtJxNQpXnXVQNY46geyL559Aaj239s26baTSf1IGPP67vEtBPQ1hbLglXv3SnO7NaD4Sd/O89gntGDedcL+iR6S1bJR2UrVFmWBTsbt3tlv3r89M2tRr/ERv3/fuPzujRY509Pq3baF3dFy4FlB0G7oUE9ZHf6qvvX6feEO6OzDY5or/aHUuD8A7rErVlEBeEcIxERkcCBkYiISCjudA1Xp4syQzot2jQ2YXRrjkRU21nWi1oySTFvFFBax3UduEtLAIDGF8+qx/u/sdnoN9K8S7W3np5UbWdkTHcqsWJ74Vwgm2qOT+mf+ejA241ecxm9mCh+Xqf6rCWx+KZSIa7GtmFl05GTb25RD3/iYz8xuhkp4LhOAbe/otOX1rj5nqpYmtKyYNVl5zz3bFcPn72p3eiW6NZXcuPhjard8pxO/2ZGx4zXVPw9vpICrtcLtKw2M21txUQKWBzEbJxaEdj7OgemfylkeMdIREQkcGAkIiISODASEREJxc0xCnLOJxTL1HMR86JpMV8YmzE3+MfEvKgr5kJlOyiu48BZ9E6mbn7pgn7igU1Gv4OtestJ/wmx33JY/55uUNti1A/09oPEFvW+kOcmdxldnojr3yk6pd+CVkps9q9UfD6WbcNu9LbETLytTT3+8d/LMy/aKfa5vqH3aFoTk8ZrKvq5EHOj1k691Wjk3V1Gt0Snnr/rPqL3vTb+6pRqZ3xxBzGvZ8X0Xla72bepWxSEcBfFCe1LosBGtedFaV3jHSMREZHAgZGIiEi45FRqTRLpF0ekaEJHbos5p9OOjaJkGgA0RfTfNW5Cb4uRW2SCXO7uOg6cBW9rTOsRXblm7OvbjX73tO9Q7e3HRHrsgn5N4ClgAHZaX6vRZIvx3FBab2+wUuLvyQpWEspHpoCnrtHp3Pd9+pDR77Pdz6j2e578Y9W+8qwuxWlNzxivqejWmGwK1e7foh6eeudGo9tyu07/don3R+xFkf71TYUEkv4VKV5LVEkCACtkn0UqDe8YiYiIBA6MREREwuWVSq1Bxurfubk8PUPAddWpGM6gLjLfMWGmgHOtOszIVYdBVUYSK4A7jujVmU8/uM/o9y+d16n2NrG60xIrgAMrjO/jRHXqsafOfI/0RfUpJ25M5H2r8CexZduwm7wU5Ow1Perx7j8cMPp9cvPPVfvzj9+u2rtH9Wus+QXjNZVL/+oUamSLXhU+v9dcIZ5s1Re07YSOzX5tQLUdUXkIQHCpVVuvuLfj9eZzhazGvwxTwLxjJCIiEjgwEhERCUylUkXIg4Yz06k8HaucpnFdlTZyT51RD2/0pX8LKsYd9ArgbAq48yW9qvSRf/ig0e+hbn02at9hnUq1RkSh/CBTwNkDn1ON+m/yGzpPGl1ubdLX9LMdYiohFkHQ5Orf+beJVOqnzffHLVtfVe3vP/Fu1d4xLVb/nl4yXlPZAhAiBdy9QT28fJV5eEKqWQ8Bjaf1Sl/rtD5wwVkw09ZBMFYA15vpXyvHIRXlTP/yjpGIiEjgwEhERCRwYCQiIhI4x0iVV+15xLVk43MSYruIrFiSo39ViblR6/iAenjreIfZr07MjYpqMY6sHBPU7+PoQ8M7jultJd/+rjkv+sDG96t27/N6K4o9OqXa6cDmRS21pSHRoee2btv+gtFLFpl/ZLPe2uM21KEaLEsfGr549Vb1+NAd5nz/27fp+d1jT16p2v3f058FZyBhvKZiBdzlvGifngudu6bX6Lbcqv8/tB/X85+RYwOqnfFvbSvyPc47RiIiIoEDIxERkcBUKtFqwpAuXctKCtg4s9DcErCyPcLrLyrfVOH3c11XLa+3Xx9Uj++Y6DT7xUX6cVJvRcnIIvpBxe+6cLMxt53Q1/lb//wBo9vXe9+r2t3P6X9W7Qmdsk47AV5zy1JpycUeHc+de582uhnnjO7aqdpOc1x/K9syXuNWqHC+ZUdgt3jF+8ffo4vMf/jzB4x+RsxPibNR/1anX61jbxivKXZrDO8YiYiIBA6MREREguUWkZKwLGsMwOCaHcun33Xd7rW75caYC1aLcV+WMQO1GTdjLlgtxr2uYi5qYCQiIlrvmEolIiISODASEREJHBiJiIgEDoxERERCURv866x6N44mAIBl6U2fTnOD0c/p1bX03tSg6xu+uqTrONrD5tlq9rzemLyyICiBBSTdZXN3aZFqPWaI7+Q/lyzZpv/3OXG9iMpO6BfVzZgbW11ZA1Ssu5rD1Hipq8qMuAV53QEA4qw1RMTfZhmxc9i3IXe1RWJlv9YBKEfMQG3GzZgLU8nPYqXU4rXOF3NRA2McTdhn3QQAsOO6MkLy2rcY/RY/rytUPH/191X7+hd/R7Ub72s3XlP3i1dUe6WY8yH3YDHhrcuY5YGd9vbtRr9zv9mj2rNv1Yd0tr6sK4ds+eGo8Rrn5IBqy2oQB9zHSl4mLeOW5HUHAHtDl46hpVG1rTlRwWV8wniNUeA7q9zXOi85uJdQTaYcMQNFxF0mgV7rMqnFmIHKfhYrpRavdb6YL7kknJPUVdrrj54xn7xvm2ru67tLtZuH9D/e/tdkknlOeS8TedcRmzL/oR17Rf+BdlvLTas+fsWUWbE9iK0ucmBc3tJmPNf4GyOq/aIczDfrwXz5Rd9gPqi/XyVPEDdO4O7bZDx34YO6Wv70m/TA0v6qPmm89yfmqQTWgD5RvKInn6/8PN/deaRDXMdGkW1Y1FmDzJR5qrub74SOoPjv1lfDLVtEBs4xEhERCRwYiYiIhEs/XUMcVpmZmDSeqvuFTjl21emU2MrBqsAqqdNKHX4pyJ8fGThvPLfr0T7VHvnZLv34iD4I0/a9JiO+X6XIdG103vx5w4N6ru5Lm/au+vieefO0haAqHRkp4G3m6Qn1v63nPU/lmM9dft18Td3QBdWuVCpVxhzZbB6OOvo+Xbl/Rp/nirbXdbvnad/74+w51Q4i/QsAVsxMQdutzfq5Bp0CdsUpHM7svPEaN1X593XBZCqYKV8KCO8YiYiIBA6MREREAgdGIiIi4dLnGCXf/KCTEF+vsv+sasQcRWbaXFpvv6TnXBpjMf1ESs+FZvzL7wOY83BTem4qcso3L/rdftU++MwN+vHzyZyvyaSCmeuSIsvm+2N0olW1n1hoXvXxXcuVn3P2k3OMiR0bjOeit+l50eM55kUTg+Zr6i7o7TQVnWO0deGJSG+P8dTUf9Fz5zM79d/Bbaf0NpmOXwwZr0mf0/O5Qcz9y+sOAHaj3tsKuW1GfP6cxUVIQc3h0uWBd4xEREQCB0YiIiKhPKnUWuRLgxolx8KU/pXbYibN9G/ssE7/dtTp9K8rtsJkEr70bwCpMcCsjBQ7ecF4bus/6spI9zx3u378TFq8xqyMlA6gMpJkOeb7Y3FZb4MYSs+v+niTU53tBLb4f7+8y0ylLvzujGr/+B3fUO0P/foPVLtxxHxNbGxctY1pkXKS6V9RIhAAFv6Tfn/MbtP/RLWK90fTf/jeH6M65kDe47avbnJcp3ytHJ9Fp0qfRSoe7xiJiIgEDoxERETC5ZtKrUX+1b9yZd4iwkWmgH0nZTQ+K1YA51h1mPGtOgwi7WQUxj9pnkrS9I96defNh+5Wbbm6s/6kb3VnwOlfAHBts2h4Y71eodwXbV71cdduRNBk+je1yywyf/Z3dcr0/+77R9X+40MfVe1d0+ZrItM6ZRxI+rfTLM6ffKteIb6wWaTXxQrxupfNQzOMqZGg0qq+ovK2/PzlWI3vVGE1frXxjpGIiEjgwEhERCQwlUoV5998nZmd1V+EqUi0TP8Om6nU9gM6tduRoxh3xleMO6j0mLy+8dPjxnOzj+ri53te1GejyuLn8dO+IhABb5bP1JsrPHu6dIrx1iZ9Tb/SNSteY6Yyze9QGVZM/3OZ2bnZeO7kx/RzH7/uOdV+5JfXq/aeb5mvseb07+ZWsqCF+IxF2s3rltmtpwgSG/X7Oj6i39eRE+YUgVEcJYjPrH8FsEjDWzkOqXBKPKSCd4xEREQCB0YiIiKBAyMREZHAOUaqrmrPK+bgP6w3MzklvprCqqr0u8g5xsz5YeO5nh/pKk49P9NzSFgUc6NTZhXChEEAABqQSURBVEWlIApyy59Rf8Y86Hzqn/Uh0TtPfEa121/Vf8d3nDF/TyeAmC0xV5dqNg+E7u3XW5K+3HNUtQ/271HtdLM5v1cnvl8l3zlyHs7Z7psbva1Jtd9x3QnV/vUvd6v2FY/45kaP6fl217+Vo1zk1pgu89Dy5b26MtJ8n/7dmof0Z7b+qFkZKTMh3mMFzDfyjpGIiEjgwEhERCQwlUpUiJCmfP38qa30iNh2Yom/g11HtIP/3WQq1R0yi8xvelKnxHqf1lV5rDmdwnN81ZQCT/+emzGeG//Xjaq94/ydqt36sk71dZwztwAFkf4FzBRwsiNuPNf9ljHVfnTnQdW+fk6nfVMdwaeAjcL4InUKAIuf16n/QznORsV95mvqfjGn2oVURuIdIxERkcCBkYiISLDcItIolmWNARhcs2P59Luu213KN2DMBavFuC/LmIHajJsxF6wW415XMRc1MBIREa13TKUSEREJHBiJiIgEDoxEREQCB0YiIiKhqA3+dVa9G0fTxU9Yvi/jehNpokePvXvb9Flxw+l64zVTI606qGmvvuOSM4ekk/B99+Ksq5h9rJjeBJtu1RuJ0816QZWVNkOpm9Ybu60lvRl81pkYL3VVWaFxy/PhrIg4ay0q2o5vUZjcEO54v0MCC0i6y4Fc63IpR8xAbcbNmAszh6ngPotlUovXOl/MRQ2McTRhn3XTRY9bUfPbWHuuUO3X/meLah/e/w3V/srEbuM1j9/3AdXuevI1AMDzM/9UTHirWk8x+w/sjG7U1TbGPtCv2+/Rh3TWDceM1/Q/pauHRF7URYP/deHhkpdJ54zbx4rpQdzuFFU1Ott0n4RZxNsZ10WAnXnvgNdDzoFLDVUpNGaDPFw5VzUZYNWKMofcgxc9dikuKe4SlCNuxlyYA+5jgX0Wy6UWr3W+mMtSEs7NmCV27AF9Ivieb21X7X0/1yeI18+ar9lw5JxqZ+a88j1uBU9Ad313JJF5fQpB00ldzf22U/p/1Atnthqv6RsVdzErdzQV3P5i2eYfN06XvmMdu0HHcvqWb6r2RYP562Iwf03cAS+UK8pVWGbcka4O1Z68cYdqj1yvr118xPwjYNuP9R8r9stveN+2tBvzosg/pOw2fd0t0Ybv1HBHnMjhLGVPswjL7igrz7XjFi66zHGOkYiISODASEREJJTndA1f6mUlFQoA1guvq3bXMZ2686dfM+JUAJ2WLEt0q/OlaZ0RXWV+6w/0QpyRY7tUu3/erIZf/9pZ1c5kU2Wuf56pgqwlPQ8XP6/nEr80ule1f3T2TcZrmqfE752pXKpaMhbYAHB6dCp1+EYdw+l887lnRQr4ZPbA3WQF/67zpRptcVjq3A0i/XutjiE+Zr5my0ExZ/rqSa9R+rqb4sgDX1ubdTzNuu2mzBSwOzOr2s7K5zIM2VWmfykgvGMkIiISODASEREJHBiJiIiE8swx+ol8vzxRPOM7XTxMnEW9v886cVq1Gwf1njv/nFw6IX6flTnLCk51+Odl3WE9L9r/A70x9uDxG1S7edac82w+qk9Lz4jfOUhWSv8e0Sn9FnxiQc97PTe5y3hNbFGeOF/5eVz/vKi7Uc8xDv1X/fNP739AtS+aFx0T86KD3kn0VqrCf4v6t8aIfaKL79yp2mPX6Dnp+KT5pt347IT+diezW+qCnBsV86J2g57vtxobdZ+0Od/vzOv9Rm7K3ANLVCzeMRIREQkcGImIiITKpFJrUY70rxum9K9vSfpKaTQAsI/q8m4dJ+S2GDPtuLKtBBDbYirMnwLGhVHV3P5DnT69Z+B21a6fNn/XjiMjqu0sZLfGOMFtjYH4UTIdOpTW/w9Gky3yFbDTwW8huCgF3KtLbp7Zr+N+5rfuVe2vjb3HeM0v5q5V7Y4LXrq+oilgf/pXbCtJXa3TvxNv1mnVi94fvxLvj8FsFa1UgOlf8TvY9frzZ8VzfxbdKnwWqTC8YyQiIhI4MBIREQlMpdYykVp1EroIOmQ7DPyVkWZ1+jH678dVu/dYkasOK1jt5KLC+GIF8LandMr05qG7Vfui1Z1H9OrOlVXPgaZ/AeNPXzemf3ZfVKcre+rm5CvgRIOtzuNP/2JTj2oO/KZOn9734e+o9rfOv8t4yWRKny7Tmj2JxZoNsDKSrCR01XbVnt6tV4vX5Vshfk63YRYiqix5BFxd3apt/2p8Z7XV+OsM7xiJiIgEDoxEREQCU6kUPJF+cRbEQZD5Cg4EXSTan/6dnFbtxud1+nf70cKKcbvJyqd/gVVSwCP6gOe+f9FFzfdM6rNRLyp+/pL+XauRAnZjOrWa7tAp9VubdAr+9c6Txmseb9SF3Y3DoyvEn/61xOrfgQ/pVPvv3PqsavsL+uPBTarZNDWjH59BYGxZNGGHPm92cYc+ZzR20eEJ+rzdzNi41whyUW2O9K9cDXxRMZTVDqnIg3eMREREAgdGIiIigQMjERGRwDlGCo8wHzYr5kUz03I+aHaVzlnV+H381ZEm9Bxjy8/03ErrET2HhKQ5N+pMTulvF8DcqH8+KDKqf37vT3UR9J2Jz6h2fMSc49t2TMznrlSUCXJetEHPdSU26+v55Z6jqt0SMbdRPd6hi8w3+besVIpt/hx7o54bHfyQLpT/5t/SB8y/cGar8Zq+h/XX8ee8NQLWfICVkVr0HG7mzdtVe2KP3hpTP2u+p1qODOvXDGXnSPNMNfKOkYiISODASEREJDCVSlSKMKd/YS5Nz4gUKeT2AP/5ltXeGjOh4+z4N/1cxxG93cRKmGcuOuM6Zexkl+a7lUz/Oub3jkzoVG73cx2qvSNyp2rXDceM1/Sf0NuTgjqswLLNtKTTrCsLLezSKeBHdx5U7a+0+c4Z7dEp4IZodgixKlct6aLC+Ns3q/bxT+otGqf336/aF52Nep84G/VJr9KTNZM7fc07RiIiIoEDIxERkWAVk26wLGsMwGDlwrlIv+u63Wt3y40xF6wW474sYwZqM27GXLBajHtdxVzUwEhERLTeMZVKREQkcGAkIiISODASEREJRe1jrLPq3Tia1u4oyGNBljfofTxv3TC2WncAwMvj3nxoanoSmYWFkjbIXDYxR/WenIzYm5Rqyz2HHJvRYS5MD42XOnl+KXFL8ndANM9bM7s3bykzh6STCPxalyKBBSTd5ZI3fdVi3Iy5MHOYqvpnsVi1eK3zxVzUwBhHE/ZZN63dUdTji27ZotpvfEq3D3/qfuRy5UPeWXFn/+5rxYS3qnUds9hUG2nXm4rn3qM3tw7tN2sGSn1P6d/5F49/vuTVYAXHLYnrHmnXG7jR05X7NaMTAIDnpx8v7met4pJiluTG5nznAGY30R9yDlz6zxJKjrtIh9yDa3daA2MuzAH3sep8FktQi9c6X8xMpRIREQmVKQknT2gfHlXtnd/Tp0XvO3EXctl5xCsJNTqV+26n7GRZrIQuz9RyRj9826ncf82s9LOTObuUn7hDsVr0SfKj79CPn77lgZwvv/K8+H9Q+s1X4eSdbquOe/G6Xap95ubcf7Nt+7F34oLzbH3OPhUl73JF/FZnx2q9AQButhybNRvQKQrFyFfOi9u56DLEO0YiIiKBAyMREZFQ8dM1nIQ+nNN69ZRqdw3EV+vuvWbJe42bCqbivPfDdMooMzWt2j1Pj6j2yPAu5NLzmtfv5FwqZ5+yk+nfZZ3DbRjVqbEvje7N+XLZL1AyBSwW3Fy4Tr8dT30kz0KnGS8FnPp1deKPNOuVc4lrxUKn98dW6w4A6Pu3DQAA5/kqpX8BI2VqN+sUsN3WulpvAICTPYi5ogfRXgqmf6mCQvZuJyIiqi4OjERERAIHRiIiIqHic4ySm9LzYJlUAfsaqjRXIE/TdgaGVLtxOHflGye7xcNNBTnHqK+PI+ZFN/1Un2Z+cOKGnC/f9JLud7TMoeUl50bTektO3ayeN3pioRm5rPSzAtzNI7doWJ3tqn3u3Xpe8fgdeeZFU9l50aNVmteFOa+YvuYK1T7/roacr9n88yUAgPvrKs2NynnReh2D1dKS8yXuXPaE9tKKItFljHeMREREAgdGIiIiIdBUai2S6V83xOlfR6R/rROnVbtjaO1tMYGTKeBsRRgA2PJTXTnmnvnbc758y6+8LQRn54LMpQoifjul03VD6fmcL1H9gn57iBSwLVLAQzfq9OlP7rw358s/GLsbAJA8UZ2/oe0GHaf75p2qPfLO3KnUnl95qVT3leqnf+WBBHZDgZ/FKn0sSeMdIxERkcCBkYiISGAqdb0Q6T25qjazHGD1oEvgLC2ptv3yG6rdO1jIqsMAfzdZGH9SrwDu++kG1b45dXfOl/dlV3demHNy9qk4keJzYvr90hfNvQJY9QtygadciSoKs5+5UVfo+fNPP5zz5X/y4B0AgNSZ6hRsl6tnccV21Zx6W/vFnbPaX9LvqWCXiK/OiukUsBXPnZJ2Vz6DqfW1Aph3jERERAIHRiIiIoGpVKouuUJVFJxHvhRw9jVutVYAz+vVp9EjOv27/XQBxbgXA05tixSwK1LAW57tUe09sdxno/Y96xWsGJ4P8FqLIvOI6nRoslXHcGtT7hXAX8j2c4PMpMr0b4coAHFjp2rv/9SzOV/+1EPv1l9UKZVqiRSwvW2Lai9ctWG17gCAptfGvdcO5i6gHxSZ/i1oBXCe9C/vGImIiAQOjERERAIHRiIiIoFzjBROYT5sVs6LZreOAObcY67XuG71tmtk5hdUO374hGrvfqNjte4AADdbmcheCPLQcH2N3OkZ1d70y42qvbPtMzlfvu2XaQDA8EKV5kXr9VzXUo+O4cs9uScPv9fzroqEtSYxNxoRc6Mj7xPX+hPHc7781Lf3AABSj1dnjtGOi7nEPdtVc+Ka3O/pziPee9o6nnsbCu8YiYiIBA6MREREAlOpROUS5vQvYGzdyGS3jwAAZvOlgJ3sfwIs2C6uY0bE1vjLk6p91amu3K8fnQBQxfTvnI6559f68R2b78z58j7R742cvSpApoBFhZu5bfrhR3cezPnyK7d5qVSnLmeX8pOF8Xv1tqM3PqrTp69/Ks/ZqA9525OW/y73fh7eMRIREQkcGImIiASrmOohlmWNARisXDgX6Xddt7uUb8CYC1aLcV+WMQO1GTdjLlgtxr2uYi5qYCQiIlrvmEolIiISODASEREJHBiJiIiEovYx1ln1bhxNl/zDrIjeN5JuN48F6dio91X1Rr39RwNnUxifzJR0NPRlGbOt/95xG8yyR8l28VxUzy8nB8+Nlzp5XmrckvwdAABR8Va1vcu7lJpBMr1Y1WtdrAQWkHSXSz7uvBbjZsyFmcNUqD6LhajFa50v5qIGxjiasM+66ZIDibTpDZgT+68ynvvw5w+o9he6vDqO19589pJ/1orLMWa7oVG1M1fvNp4b3K+fS/amVPvMJ79Y8mqwUuM2zrRrbjaesjfoc+3cuLeb+PlTf3/pPyur5JjzEb/PykbqQ5l/Lcu3rmjcqzjk5t7kXSjGXJgD7mPV/ywWqRavdb6YA618Iwsub3jmnPHcD2ZvVO3HWz8AAHj13NeCCSwPN51W7cbRtPHcN4/eoNr/sc0rFXFy+fvBBJaHPHB0Znej8dwnf/viwRwAgjzTNRdbxO1e2W88d/pmfQhwYqNXhSVxbwgKN8nBvKHBeMruFIWM67wiy9ZQkCVCLpG1yh/RXLxOlxHOMRIREQkcGImIiIRAc1EyLZkZOm881zQ2odrN2QUv0fmlYALLw13SMTS+aM4f9n9js2qPNO8CAKSHc5/xFZiMLvgcnzKLPz868HbVnsvIxUSBli5elSVSkZNvbjGe+8THfqLaaj73G2PBBJaHVSdSo+I8OAA4e5M+3y7R7eUil/+mOufWXUSmgOvN96zVptPWViybAh4OSdy5MP1LZcQ7RiIiIoEDIxERkcCBkYiISKjaenc537ja1wDgisM/q0XGlR4x57Ri4rDXWHZe1J5PBBNYHs7iomo3v3TBfPKBTap5sPUG8cQ/VTiqAoj/37FF8//9c5O7VPuJuPc7TTvV/7vObtTbYSbe1mY89/HfW2Ve9JHqz4sC5tyotXOb8dzIu/UhwIlOb+4u+a1wbTOxYmY8drPYGJ4tBmFNhmA7D9Wk6v/LQkREFCIcGImIiATmGorhmFsfZMpyRdjSv5lzZiq1cXJatZsi4fq7yFnQW2Naj4waz419fbtq39O+AwBwfqT6lZEkO23uDxhN6i0nQ+l5AEDSNd9D1SJTwFPXdBrPve/Th1T7s93PAAD2PzkeTGB5yPSp3b/FeG7qnRtVe7ndS/+mvheCrVOCFTX/uZXbkyz5WZwKKiLKJVz/MhIREVUZB0YiIiKBqdR17qLVv6KQe9i4qaRqO4NmkfmOCZ0CXll1eHpqOZC48pHp9I4jk8ZzTz+4T7X/pfM6AMDA2FeDCawITtSsGtNTp98jfVHvlJM6q/r5PbtJpx5nr+kxnuv+wwHV/uTmnwMA7n5mAtUm06eRLZuM5+b36q+TreIe5ZGKh1Uc2zxiwI6LFHV2Nb41v77usdbXb0NERFQiDoxEREQCU6kUSjKtCgCZ6dTFfZyLi0IEzU3qON1TZ4znNor070ox7vMT5u9VLTIF3PnSjPHcI//wQdV+qNs7G3VoIgQrgC39d3yq0fyb/obOk6p9a5O3Avgv7OqvEJerf+ff5kulflq/P27Z+qpqHw5BKtVIAXdvMJ5bvkofnpBq9vo5PyvPAdzlctEKYFEo3yog/cs7RiIiIoEDIxERkcCBkYiISOAcI9UGN6Snzoq4nISvgPzyxdtJXOfiudJqkHOj1vEB47mt4x36izpvbnRsvPpxy0PDO46Z246+/V09L/rAxvcDAC5M/XUwgeUT0VsdEh3mtofbtr+g2itF5gHgLysf1ZpkVZ7Fq7cazw3dod8Lb9/mze1GX6/+1iljXrRvs/Hc3DW9qr3c6v1/SD+Ve16Ud4xEREQCB0YiIiKBqVSiSglr+hcwU8C+YviOSFmubJFwM9XfZuKI1LT9+qDx3I4JXQjdjXvFxqfGQrCdR8TcdsK8zt/65w+o9td73yue+WKlw1qTTEsu9pjDxJ17n1Ztdc5offUratktumj/+HvMIvMf/vwB1VYxv5D7bFTeMRIREQkcGImIiATLLSLdY1nWGIDBNTuWT7/rut2lfAPGXLBajPuyjBmozbgZc8FqMe51FXNRAyMREdF6x1QqERGRwIGRiIhI4MBIREQkcGAkIiISitrgX2fVu3E0leUHWxFzTE6369p8b9ribbwcOJvC+GTGKuXnXO4xw/KF0hhXzeUN+rnk4LnxUleVlTVuH8sW1z5bv3MpNYNkejE817oACSwg6S6XFDNQm3Ez5sLMYSrUn8XV1OK1zhdzUQNjHE3YZ91UlqAiza3G1xP736Lah/7qfgDAtTefLfnnXO4xywM6AcC5+irVPnFnTLXPfPKLJS+TLmfc/gHdbm7WT/V5BYGfP/X3Jf+Yssacj+0VLj6UKc+BroHFnXXIPVjy92DMhTngPhauz2IBavFa54u5aiXh/GWoNjxzTrX3ffEuAMCr50Jwarjgps0SU42j+uvbTnn/Q08ufz/QmNZi+wbGqd36RPHTt9yv2mbd/+qz6uqMr90r+1X79Tu8vyoT94asoqF/MBcnFNi9PV6XIfP3Cr2V34m7uugywjlGIiIigQMjERGRULVclD8tmRk6r9pdT0wDAKLzSwgTJ2Eextn4op5PHPnKLgBAethMXVabm8kYX8en9NdfGt0rnnkjoIgK408BT7xZV84/9ZHsfO43clfHrwYrGjMf2LNdNU98rA0AsPw3vj4hZMf1Ai17QxcAwBoOf9wK079UIt4xEhERCRwYiYiIBA6MREREQmjWu8s5x8zsrPeY61QrnNU55nxdekTPcTU+vQAAsOcTgYa0FnmCOAA0v3RBtQ/ee4N45p8Ciqgw/lNfYov6vfDEgrencdoJ1991dkPc+HribW2qffyO7LzoI+GaFwUAK2ZuIbF2blPtgf+2AQCQ/Fa4t5nI38Fu9667NRmaf96oxoTrXxYiIqIq48BIREQkMNdQCpFadebmAIQv/XvRtphzOpXa8cPZoMMpmLtkbtVpPTKq2vd89XYAwPmRkFVG8qV/7bT+eig9DwBIumY6PgysuK860jWdqv3ju+4FAOx/cjzQmNZiRc1/uuz+Lao9/EGvZGDqe+HaOuUnfwe7WdQInapCMGTgHSMREZHAgZGIiEhgKvUyY6z+nZ6pYiT5+VPAzqAuMt/7mJeWPD1lrritNtdXGanjyKRq33z/3QCAgbGvBhpTISxf8XMnqr/ui3orgOuscOX3LFGgHQBmr+lR7T/93HcAAHc/MxFoTGvxp38jWzap9uQNOhWMR4KK6BLZ+sgBu8k7lMCaX1/3WOvrtyEiIioRB0YiIiKBqVSqCW4qqdqZcW+FpOukc3WvChkjALinzqj29u94BSDOT5h9wsBZMotSdL6kU+x7HvbORh2aCNcKYH/6N9Wo/8a/tclLtf+FHa4V4v5Dw+ffplOpN939nGofDlsq1TZPa41u7Fbt+f/sFYNwflaeA7grxVgB3Lh2+pd3jERERAIHRiIiIoEDIxERkcA5Rqo9bm2cQOsk9Nydc847iNt1UtUKJyc3bcZkHR9Q7d0PetsgxsbDFbfjK47fcWxOtXc+9hkAwIWpvw40prVYEXOuLtGhv/5yz1HV/svAIiqM7auMtHj1VtXe+IWTAIDo6+HaOnXR1pi+zao9/h5va0z6qdzzorxjJCIiEjgwEhERCUylEgUhzOlfX2zO4qJuD5z1umTCtc3ETZrx2K8PqvZV93tFxKfGwrWdx5/+bTuhr/OOH90pnvliQBEVxp+WXOzRXz+68yAA4Nr6OYTJypaMFSvpUwA49FfZs1FfyH02Ku8YiYiIBA6MREREguU/Qy5vZ8saAzC4Zsfy6Xddt3vtbrkx5oLVYtyXZcxAbcbNmAtWi3Gvq5iLGhiJiIjWO6ZSiYiIBA6MREREAgdGIiIigQMjERGRwIGRiIhI4MBIREQkcGAkIiISODASEREJHBiJiIiE/w+OFGPJPEAnRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 90 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = env.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_state_value(action_values):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def plot_action_values(action_values):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    \n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "  \n",
        "def random_policy(q):\n",
        "  return np.random.randint(4)\n",
        "\n",
        "def plot_greedy_policy(grid, q):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(q, axis=2)\n",
        "  grid.plot_grid()\n",
        "  plt.hold('on')\n",
        "  for i in range(9):\n",
        "    for j in range(10):\n",
        "      action_name = action_names[greedy_actions[i,j]]\n",
        "      plt.text(j, i, action_name, ha='center', va='center')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# Part 1: Implement Agents\n",
        "\n",
        "Each agent, should implement a step function:\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get the initial observation by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`.\n",
        "\n",
        "All agents should be in pure Python - so you cannot use TensorFlow to, e.g., compute gradients.  Using `numpy` is fine.\n",
        "\n",
        "### `step(self, reward, discount, next_observation, ...)`:\n",
        "where `...` indicates there could be other inputs (discussed below).  The step should update the internal values, and return a new action to take.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" (for whatever definition of $v$ is appropriate) in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "### `q_values()`:\n",
        "\n",
        "Tabular agents implement a function `q_values()` returning a matrix of Q values of shape: (`number_of_states`, `number_of_actions`)\n",
        "\n",
        "### `q_values(state)`:\n",
        "\n",
        "Agents with Linear function approximation implement a method `q_values(state)` returning an array of Q values of shape: (`number_of_actions`)\n",
        "\n",
        "\n",
        "### A note on the initial action\n",
        "Normally, you would also have to implement a method that gives the initial action, based on the initial state.  As in the previous assignment you can use the action `0` (which corresponds to `up`) as initial action, so that otherwise we do not have to worry about this.  Note that this initial action is only executed once, and the beginning of the first episode---not at the beginning of each episode.\n",
        "\n",
        "Q-learning and it's variants needs to remember the last action in order to update its value when they see the next state.  In the `__init__`, make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._action = 0\n",
        "  (...)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "source": [
        "\n",
        "# Part 1: Implement Agents\n",
        "\n",
        "We are going to implement 5 agent:\n",
        "- Online Tabular Q-learning\n",
        "- Tabular Experience Replay\n",
        "- Tabular Dyna-Q (with a Tabular model)\n",
        "- Experience Replay with linear function approximation\n",
        "- Dyna-Q with linear function approximation (with a linear model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIgXk6LblHgV"
      },
      "source": [
        "## 1.1 Tabular Model\n",
        "**[5 pts]** Implement a trainable tabular Model of the environment.\n",
        "\n",
        "The Model should implement: \n",
        "* a *next_state* method, taking a state and action and returning the next state in the environment.\n",
        "* a *reward* method, taking a state and action and returning the immediate reward associated to execution that action in that state.\n",
        "* a *discount* method, taking a state and action and returning the discount associated to execution that action in that state.\n",
        "* a *transition* method, taking a state and an action and returning both the next state and the reward associated to that transition.\n",
        "* a *update* method, taking a full transition *(state, action, reward, next_state)* and updating the model (in its reward, discount and next_state component)\n",
        "\n",
        "Given that the environment is deterministic and tabular the model will basically reduce to a simple lookup table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13zx3tTrll1g"
      },
      "source": [
        "# MDP Model with P_ss' and R_s,a defined\n",
        "class TabularModel(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions):\n",
        "    self.S = np.zeros((number_of_states, number_of_actions), dtype = 'int8')\n",
        "    self.R = np.zeros((number_of_states, number_of_actions), dtype='float16')\n",
        "    self.G = np.zeros((number_of_states, number_of_actions), dtype='float32')\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    return self.S[s, a]\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    return self.R[s, a]\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    return self.G[s, a]\n",
        "  \n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action), \n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "  \n",
        "  def update(self, state, action, reward, discount, next_state):\n",
        "    self.S[state, action] = next_state\n",
        "    self.R[state, action] = reward\n",
        "    self.G[state, action] = discount"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPBcz1riy_fD"
      },
      "source": [
        "## 1.2 Linear Model\n",
        "\n",
        "**[5 pts]** Implement a trainable linear model of the environment.\n",
        "\n",
        "The Model should implement: \n",
        "* a *next_state* method, taking a state and action and returning the predicted next state in the environment.\n",
        "* a *reward* method, taking a state and action and returning the predicted immediate reward associated to execution that action in that state.\n",
        "* a *discount* method, taking a state and action and returning the predicted discount associated to execution that action in that state.\n",
        "* a *transition* method, taking a state and an action and returning both the next state and the reward associated to that transition.\n",
        "* a *update* method, taking a full transition *(state, action, reward, next_state)* and updating the model (in its reward, discount and next_state component)\n",
        "\n",
        "For each selected action, the predicted reward, discount and next state will all be a linear function of the state.\n",
        "* $\\text{s'} = T_a s$\n",
        "* $\\text{r'} = R_a s$\n",
        "* $\\text{g'} = G_a s$\n",
        "\n",
        "Where $T_a$ is a matrix of shape $(\\text{number_of_features}, \\text{number_of_features})$, $R_a$ and $G_a$are vectors of shape $(\\text{number_of_features},)$\n",
        "\n",
        "The parameters of all these linear transformations must be trained by gradient descent. Write down the update to the parameters of the models and implement the update in the model below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226SA-xjlyDe"
      },
      "source": [
        "# There is no table, state is a vector of values. Rewards and Returns are always scalars.\n",
        "# In particular a state is defined by a set of features\n",
        "\n",
        "class LinearModel(object):\n",
        "\n",
        "  def __init__(self, number_of_features, number_of_actions):\n",
        "    self.T = np.zeros((number_of_actions, number_of_features, number_of_features), dtype = 'float32')\n",
        "    self.R = np.zeros((number_of_actions, number_of_features), dtype='float16')\n",
        "    self.G = np.zeros((number_of_actions, number_of_features), dtype='float32')\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    return np.matmul(self.T[a], s)\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    return np.matmul(self.R[a], s)\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    return np.matmul(self.G[a], s)\n",
        "\n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action),\n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "\n",
        "  def update(self, state, action, reward, discount, next_state, step_size=0.1):\n",
        "    self.T[action] += step_size*np.matmul((next_state - self.next_state(state, action).reshape(-1,1),\n",
        "                                           state.reshape(1,-1)))\n",
        "    self.R[action] = step_size * (reward - self.reward(state, action)) * state\n",
        "    self.G[action] = step_size * (reward - self.discount(state, action)) * state"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Exqjy3QcLVj"
      },
      "source": [
        "class ReplayBuffer(object):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.buffer = []\r\n",
        "  \r\n",
        "  def append_transaction(self, transaction):\r\n",
        "    self.buffer.append(transaction)\r\n",
        "\r\n",
        "  def sample_transaction(self):\r\n",
        "    return random.choice(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omzJxb5ds0Iq"
      },
      "source": [
        "## 1.3 Experience Replay\n",
        "\n",
        "**[10 pts]** Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning \n",
        "* apply multiple Q-learning updates based on transitions sampled (uniformly) from the *replay buffer* (in addition to the online updates).\n",
        "\n",
        "**Initialize** $Q(s, a)$ and $\\text{Model}(s, a)$ for all s  S and a  A(s)\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{random_action}(S)$\n",
        " \n",
        "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a)  Q(S, A))$\n",
        "\n",
        "5. $\\text{ReplayBuffer}.\\text{append_transition}(S, A, R, \\gamma, S')$\n",
        "\n",
        "6. Loop repeat n times:\n",
        "\n",
        "  1. $S, A, R, \\gamma, S' \\gets \\text{ReplayBuffer}.\\text{sample_transition}()$\n",
        "  \n",
        "  4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a)  Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9e_reb2pJX"
      },
      "source": [
        "class ExperienceQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    \n",
        "    self.action_values = np.zeros((number_of_states, number_of_actions))\n",
        "    self._numbre_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    # target policy is simply greedy w.r.t. to action values\n",
        "    self.num_offline_updates = num_offline_updates\n",
        "    self.replay_buffer = ReplayBuffer() # supervised dataset\n",
        "    \n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self.Q\n",
        "\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    self.Q[s, a] += self._step_size * (reward + discount * np.max(self.Q[next_s])\\\n",
        "                    - self.Q[next_s, a])\n",
        "      \n",
        "    self.replay_buffer.append_transaction((s, a, r, g, next_s))\n",
        "    for _ in self.num_offline_updates:\n",
        "      s, a, r, g, next_s = self.replay_buffer.sample_transaction()\n",
        "      self.Q[s, a] += \\\n",
        "      self._step_size * (reward + discount * np.max(self.Q[next_s])\\\n",
        "                          - self.Q[s, a])\n",
        "      \n",
        "    self._action = self._behaviour_policy[self.Q[next_s]]\n",
        "    self._state = next_state\n",
        "    return self._action"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfA7ifHvO-M"
      },
      "source": [
        "\n",
        "## 1.4 Dyna-Q\n",
        "**[10 pts]** Implement an agent that uses **Dyna-Q** to learn action values.  \n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning to Q-value\n",
        "* apply an update to the *model* based on the latest transition\n",
        "* apply multiple Q-learning updates based on transitions *(s, a, model.reward(s), model.next_state(s))* for some previous state and action pair *(s, a)*.\n",
        "\n",
        "**Initialize** $Q(s, a)$ and $\\text{Model}(s, a)$ for all s  S and a  A(s)\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{random_action}(S)$\n",
        " \n",
        "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a)  Q(S, A))$\n",
        "\n",
        "5. $\\text{ReplayBuffer}.\\text{append_transition}(S, A)$\n",
        "\n",
        "6. $\\text{Model}.\\text{update}(S, A, R, \\gamma, S')$\n",
        "\n",
        "6. Loop repeat n times:\n",
        "\n",
        "  1. $S, A \\gets \\text{ReplayBuffer}.\\text{sample_transition}()$\n",
        "  \n",
        "  2. $R, \\gamma, S' \\gets \\text{Model}.\\text{transition}(S, A)$\n",
        "  \n",
        "  3. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a)  Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdJgVK6_3Q3-"
      },
      "source": [
        "class DynaQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    \n",
        "    self.Q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._numbre_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    # target policy is simply greedy w.r.t. to action values\n",
        "    self.num_offline_updates = num_offline_updates\n",
        "    self.replay_buffer = ReplayBuffer() # supervised dataset\n",
        "    self.model = TabularModel(number_of_states, number_of_actions)\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self.Q\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    self.Q[s, a] += self._step_size * (reward + discount * np.max(self.Q[next_s])\\\n",
        "                    - self.Q[s, a])\n",
        "      \n",
        "    self.replay_buffer.append_transaction((s, a))\n",
        "    self.model.update(s, a, r, g, next_state)\n",
        "\n",
        "    for _ in range(self.num_offline_updates):\n",
        "      s, a = replay_buffer.sample_transaction()\n",
        "      r, g, next_s =  self.model.transition(s, a)\n",
        "      self.Q[s, a] += self._step_size * (reward + discount * np.max(self.Q[next_s])\\\n",
        "                    - self.Q[s, a])\n",
        "      \n",
        "    self._action = self._behaviour_policy[self.Q[next_s]]\n",
        "    self._state = next_state\n",
        "    return self._action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra01mmV5VPgm"
      },
      "source": [
        "## 1.5 Experience Replay with Linear Function Approximation\n",
        "\n",
        "**[10 pts]** Implement an agent that uses **Experience Replay** to learn action values as a linear function approximation over a given set of features.\n",
        "\n",
        "**Training**: To make sure of the experience in an online fashion, we will learn this linear model via gradient descent. Write down the update to the parameters of the value function and implement the update in the agent below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOy_bpVa3j6V"
      },
      "source": [
        "class FeatureExperienceQ(ExperienceQ):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_features, number_of_actions, *args, **kwargs):\n",
        "    super(FeatureExperienceQ, self).__init__(\n",
        "        number_of_actions=number_of_actions, *args, **kwargs)\n",
        "    pass\n",
        "\n",
        "  def q(self, state):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlu3YPGAO9ss"
      },
      "source": [
        "\n",
        "## 1.6 Dyna-Q with Linear Function Approximation\n",
        "\n",
        "**[10 pts]** Implement an agent that uses **Dyna-Q** that uses a linear function approximation to represent the value functions and a learnt linear model of the environment (represent and learn both the **transition model**(action conditioned) and the **reward model** as linear transformations of the given set of features).  \n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning to Q-value\n",
        "* apply an update to the *model* based on the latest transition, use a step_size of 0.01\n",
        "* apply multiple Q-learning updates based on transitions *(s, a, model.reward(s), model.next_state(s))* for some previous state and action pair *(s, a)*.\n",
        "\n",
        "**Initialize** $Q(s, a)$ and $\\text{Model}(s, a)$ for all s  S and a  A(s)\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{random_action}(S)$\n",
        " \n",
        "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a)  Q(S, A))$\n",
        "\n",
        "5. $\\text{ReplayBuffer}.\\text{append_transition}(S, A)$\n",
        "\n",
        "6. $\\text{Model}.\\text{update}(S, A, R, \\gamma, S')$\n",
        "\n",
        "6. Loop repeat n times:\n",
        "\n",
        "  1. $S, A \\gets \\text{ReplayBuffer}.\\text{sample_transition}()$\n",
        "  \n",
        "  2. $R, \\gamma, S' \\gets \\text{Model}.\\text{transition}(S, A)$\n",
        "  \n",
        "  3. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a)  Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RxFwgIU39dI"
      },
      "source": [
        "class FeatureDynaQ(DynaQ):\n",
        "\n",
        "  def __init__(self, number_of_features, number_of_actions, *args, **kwargs):\n",
        "    super(FeatureDynaQ, self).__init__(\n",
        "        number_of_actions=number_of_actions, *args, **kwargs)\n",
        "    pass\n",
        "\n",
        "  def q(self, state):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "# Assignment 2: Analyse Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AWyVs16A-x"
      },
      "source": [
        "## 2.1 Tabular Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qekcmj4R5Y6J"
      },
      "source": [
        "### 2.1.1 Data Efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWutE_URvT7K"
      },
      "source": [
        "**Online Q-learning**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iix-yw-MKS4Y"
      },
      "source": [
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZXsXJYBgC_N"
      },
      "source": [
        "**Experience Replay**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASml5uAeIl4A"
      },
      "source": [
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVs2rrpwL4H"
      },
      "source": [
        "**DynaQ**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG-_cjw-wRzm"
      },
      "source": [
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujZNsXFY52fi"
      },
      "source": [
        "### 2.1.2 Computational Cost\n",
        "\n",
        "What if sampling from the environment is cheap and I don't care about data efficiency but only care about the number of updates to the model?\n",
        "\n",
        "How do Online Q-learning, ExperienceReplay and Dyna-Q compare if I apply the same number of total updates?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAdl2r7NwUF4"
      },
      "source": [
        "**Online Q-learning**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $3e4$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVVWtGoUwiAe"
      },
      "source": [
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(3e4))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoO6dEZqftB1"
      },
      "source": [
        "**ExperienceReplay**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuoUs8xVxady"
      },
      "source": [
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U-4sam12y95"
      },
      "source": [
        "**DynaQ**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDOW4dd221L6"
      },
      "source": [
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLluo2AXMsH"
      },
      "source": [
        "## 2.3 Linear function approximation\n",
        "\n",
        "We will now consider the $\\text{FeatureGrid}$ domain.\n",
        "\n",
        "And evaluate $\\text{Q-learning}$, $\\text{Experience Replay}$ and $\\text{DynaQ}$, in the context of linear function approximation.\n",
        "\n",
        "All experiments are run for $\\text{number_of_steps}$ = $1e5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSrpV6pgcCR-"
      },
      "source": [
        "**Online Q-learning with Linear Function Approximation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwlRPm1uXMyv"
      },
      "source": [
        "grid = FeatureGrid()\n",
        "\n",
        "agent = FeatureExperienceQ(\n",
        "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
        "  num_offline_updates=0, step_size=0.01, behaviour_policy=random_policy)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i)) for i in xrange(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LguqeKO4dXBH"
      },
      "source": [
        "**Experience Replay with Linear Function Approximation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb6XeKzXcIsi"
      },
      "source": [
        "grid = FeatureGrid()\n",
        "\n",
        "agent = FeatureExperienceQ(\n",
        "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
        "  num_offline_updates=10, step_size=0.01, behaviour_policy=random_policy)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i)) for i in xrange(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERjyViuj1EkS"
      },
      "source": [
        "**DynaQ with Linear Function Approximation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVDRVknH1MXw"
      },
      "source": [
        "grid = FeatureGrid()\n",
        "\n",
        "agent = FeatureDynaQ(\n",
        "  number_of_features=grid.number_of_features, \n",
        "  number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, \n",
        "  initial_state=grid.get_obs(),\n",
        "  num_offline_updates=10, \n",
        "  step_size=0.01,\n",
        "  behaviour_policy=random_policy)\n",
        "\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i)) for i in xrange(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arP0Nf0XUGrB"
      },
      "source": [
        "## 2.4 Non stationary Environments\n",
        "\n",
        "We now consider a non-stationary setting where after `pretrain_steps` in the environment, the goal is moved to a new location (from the top-right of the grid to the bottom-left).\n",
        "\n",
        "The agent is allowed to continue training for a (shorter) amount of time in this new setting, and then we evaluate the value estimates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjhd4kylg-BA"
      },
      "source": [
        "pretrain_steps = 2e4\n",
        "new_env_steps = pretrain_steps / 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1kJZmOgX7du"
      },
      "source": [
        "**Online Q-learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9lC323X7uH"
      },
      "source": [
        "# Train on first environment\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "# plot_state_value(q)\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(new_env_steps))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0UeIltFc3lR"
      },
      "source": [
        "**Experience Replay**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq5msw1iY-Q5"
      },
      "source": [
        "# Train on first environment\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "# plot_state_value(q)\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(new_env_steps))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPHiZc0-X26F"
      },
      "source": [
        "**Dyna**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwztU4EbUXe0"
      },
      "source": [
        "# Train on first environment\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "# plot_state_value(q)\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(new_env_steps))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGptHwE23lmP"
      },
      "source": [
        "## Questions\n",
        "\n",
        "### Basic Tabular Learning\n",
        "\n",
        "**[5 pts]** Why is the ExperienceReplay agent so much more data efficient than online Q-learning?\n",
        "\n",
        "**[5 pts]** If we run the experiments for the same number of updates, rather than the same number of steps in the environment, which among online Q-learning and Experience Replay performs better? Why?\n",
        "\n",
        "**[5 pts]** Which among online Q-learning and Dyna-Q is more data efficient? why?\n",
        "\n",
        "**[5 pts]** If we run the experiments for the same number of updates, rather than the same number of steps in the environment, which among online Q-learning and Dyna-Q performs better? Why?\n",
        "\n",
        "### Linear function approximation\n",
        "\n",
        "**[5 pts]** The value estimates with function approximation are considerably more blurry than in the tabular setting despite more training steps and interactions with the environment, why is this the case?\n",
        "\n",
        "**[5 pts]** Inspect the policies derived by training agents with linear function approximation on `FeatureGrid` (as shown by `plot_greedy_policy`). How does this compare to the optimal policy? Are there any inconsistencies you can spot? What is the reason of these?\n",
        "\n",
        "### Learning in a non stationary environment\n",
        "\n",
        "Consider now the tabular but non-stationary setting of section 2.4.\n",
        "\n",
        "After an initial pretraining phase, the goal location is moved to a new location, where the agent is allowed to train for some (shorter) time.\n",
        "\n",
        "**[10 pts]** Compare the value estimates of online Q-learning and Experience Replay, after training also on the new goal location, explain what you see. \n",
        "\n",
        "**[10 pts]** Compare the value estimates of online Q-learning and Dyna-Q, after training also on the new goal location, explain what you see.\n",
        "\n",
        "Back up your observations with visualizations of the value/policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9MNyXV165oG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}